{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":482,"sourceType":"datasetVersion","datasetId":228}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß† Diabetes Prediction using Logistic Regression\nThis project uses the **Pima Indians Diabetes Dataset** to build a binary classification model that predicts whether a patient is likely to have diabetes.\n\nKey steps:\n- Data cleaning (handling zero entries in medical fields)\n- Exploratory Data Analysis (EDA)\n- Model training using Logistic Regression\n- Saving predictions to CSV for external use\n\nTool: **Kaggle Notebook** | Language: **Python** | Model: **Scikit-learn**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T16:24:44.358906Z","iopub.execute_input":"2025-07-30T16:24:44.359226Z","iopub.status.idle":"2025-07-30T16:24:44.368966Z","shell.execute_reply.started":"2025-07-30T16:24:44.359201Z","shell.execute_reply":"2025-07-30T16:24:44.367841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T16:24:50.707607Z","iopub.execute_input":"2025-07-30T16:24:50.707915Z","iopub.status.idle":"2025-07-30T16:24:50.752658Z","shell.execute_reply.started":"2025-07-30T16:24:50.707894Z","shell.execute_reply":"2025-07-30T16:24:50.751555Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üîç Data Overview and Cleaning\nWe checked for missing values and found hidden missing data in the form of zeros in medical columns like `Insulin` and `BMI`. We replaced these with median values.","metadata":{}},{"cell_type":"code","source":"# Shape of the dataset\nprint(\"Dataset shape:\", df.shape)\n\n# Basic info\ndf.info()\n\n# Check for missing values\nprint(\"\\nMissing values:\\n\", df.isnull().sum())\n\n# Check for zero values in certain columns (zero might be invalid)\nzero_check_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\nprint(\"\\nZeros in key columns:\")\nprint((df[zero_check_cols] == 0).sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T16:29:49.047980Z","iopub.execute_input":"2025-07-30T16:29:49.048657Z","iopub.status.idle":"2025-07-30T16:29:49.081307Z","shell.execute_reply.started":"2025-07-30T16:29:49.048628Z","shell.execute_reply":"2025-07-30T16:29:49.080335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Replace 0s with NaN in relevant columns\ncols_to_clean = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\ndf[cols_to_clean] = df[cols_to_clean].replace(0, np.nan)\n\n# Fill NaNs with median values of each column\ndf[cols_to_clean] = df[cols_to_clean].fillna(df[cols_to_clean].median())\n\n# Double check\ndf[cols_to_clean].isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T16:31:55.669501Z","iopub.execute_input":"2025-07-30T16:31:55.669842Z","iopub.status.idle":"2025-07-30T16:31:55.692256Z","shell.execute_reply.started":"2025-07-30T16:31:55.669816Z","shell.execute_reply":"2025-07-30T16:31:55.691235Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ‚öôÔ∏è Feature Scaling and Model Training\nWe split the dataset into training and testing sets, scaled the features using `StandardScaler`, and trained a Logistic Regression model.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Features and target\nX = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T16:33:58.301858Z","iopub.execute_input":"2025-07-30T16:33:58.302192Z","iopub.status.idle":"2025-07-30T16:33:59.165970Z","shell.execute_reply.started":"2025-07-30T16:33:58.302167Z","shell.execute_reply":"2025-07-30T16:33:59.164953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Model training\nmodel = LogisticRegression()\nmodel.fit(X_train_scaled, y_train)\n\n# Predict\ny_pred = model.predict(X_test_scaled)\n\n# Evaluate\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T16:34:35.781197Z","iopub.execute_input":"2025-07-30T16:34:35.781680Z","iopub.status.idle":"2025-07-30T16:34:36.019069Z","shell.execute_reply.started":"2025-07-30T16:34:35.781655Z","shell.execute_reply":"2025-07-30T16:34:36.016902Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìà Evaluation & Prediction Output\nThe model achieved an accuracy of **75.3%** on the test set. We saved predictions in a downloadable CSV file for future use or deployment.","metadata":{}},{"cell_type":"code","source":"# Save predictions to CSV\nsubmission = pd.DataFrame({\n    'Index': X_test.index,\n    'Predicted_Outcome': y_pred\n})\n\nsubmission.to_csv('logistic_regression_predictions.csv', index=False)\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T16:36:21.889501Z","iopub.execute_input":"2025-07-30T16:36:21.889889Z","iopub.status.idle":"2025-07-30T16:36:21.907956Z","shell.execute_reply.started":"2025-07-30T16:36:21.889860Z","shell.execute_reply":"2025-07-30T16:36:21.906761Z"}},"outputs":[],"execution_count":null}]}